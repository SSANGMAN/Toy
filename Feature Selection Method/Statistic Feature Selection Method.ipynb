{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [How to Choose a Feature Selection Method For Machine Learning](https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/)\n",
    "\n",
    "\n",
    "\n",
    "Feature Selection은 예측 모델을 개발할 때 입력 변수 수를 줄이는 과정이다.\n",
    "\n",
    "입력 변수의 수를 줄임으로써 모델링의 계산 비용을 절감하고 경우에 따라 모델의 성능을 향상시키는 것이 바람직하다.\n",
    "\n",
    "통계학 기반 Feature Selection 방법에는 각 입력 변수와 목표 변수 사이의 관계를 평가하고 목표 변수와 가장 강력한 관계를 갖는 입력 변수를 선택한다. 입력 변수와 출력 변수 의 데이터 유형에 따라 다르다. 이 포스트에서는 각기 다른 입력 변수와 출력 변수의 유형에 따른 필터 기반 Feature Selection 방법에 대해 소개한다.\n",
    "\n",
    "이 포스트를 통해 다음과 같은 사실을 알 수 있다.\n",
    "\n",
    "- Feature Selection 기법에는 크게 Supervised Learning, Unsupervised Learning 두 가지 유형이 존재한다.\n",
    "- 필터 기반 Feature Selection 방법은 통계적 방법을 사용하여 가장 관련성이 높은 feature를 선택하기 위해 필터링할 수 있는 입력 변수 사이의 <b>상관관계</b>나 <b>의존성</b>을 점수화한다.\n",
    "- Feature Selection에 대한 통계적 측정은 입력 변수의 데이터 유형과 목표 변수에 기반하여 신중히 선택해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "이 포스트는 총 4개의 파트로 나뉜다.\n",
    "\n",
    "1. Feature Selection 방법\n",
    "\n",
    "2. 통계학 기반 필터 Feature Selection 방법\n",
    "    - 수치형 입력, 수치형 출력\n",
    "    - 수치형 입력, 카테고리 출력\n",
    "    - 카테고리 입력, 수치형 출력\n",
    "    - 카테고리 입력, 카테고리 출력\n",
    "\n",
    "3. Feature Selection과 관련된 Tip과 Trick\n",
    "    - 상관관계 통계량\n",
    "    - 선택 방법\n",
    "    - 변수 변환\n",
    "    - 최고의 방법\n",
    "    \n",
    "4. 예제\n",
    "    - 회귀 문제에서 Feature Selection\n",
    "    - 분류 문제에서 Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 변수 선택 방법\n",
    "\n",
    "변수 선택은 목표 변수를 예측하기 위해 모델에 가장 유용하다고 생각되는 입력 변수의 수를 줄이기 위한 것이다.\n",
    "\n",
    "<blockquote>변수 선택은 주로 모델에 불필요한 정보 또는 중복되는 입력 변수를 제거하는 데 초점을 맞춘다.</blockquote>\n",
    "- Page 488, Applied Predictive Modeling, 2013.\n",
    "\n",
    "일부 예측 모델링 문제는 모델의 개발 및 훈련을 지연시키는 변수가 많고 시스템 메모리가 많이 필요할 때다.또한 목표 변수와 관련이 없는 입력 변수를 포함하면 일부 모델의 성능이 저하될 수 있다.\n",
    "\n",
    "<blockquote> 많은 모형, 특히 회귀 기울기와 절편에 기초한 모델은 모델의 모든 항에 대한 모수를 추정할 것이다. 이 때문에 불필요한 정보를 담은 변수의 존재는 예측의 불확실성을 더하고 모델의 전체적인 효과를 감소시킬 수 있다.</blockquote>\n",
    "- Page 488, Applied Predictive Modeling, 2013.\n",
    "\n",
    "변수 선택 방법은 Supervised Learning과 Unsupervised Learning에서 아이디어를 얻을 수 있다.\n",
    "\n",
    "<blockquote> 변수 선택에 있어서 중요한 구분 방법은 Supervised Learning과 Unsupervised Learning이다. 입력 변수를 제거하는 동안 결과가 무시되면 이 방법은 Unsupervised Learning이다.</blockquote>\n",
    "- Page 488, Applied Predictive Modeling, 2013.\n",
    "\n",
    "이 차이는 목표 변수를 기준으로 변수를 선택했는지 여부와 관련이 있다. Unsupervised Learning 변수 선택은 상관관계를 이용하여 중복 변수를 제거하는 방법 등 목표 변수를 무시한다. 반면, Supervised Learning 변수 선택은 관계 없는 변수를 제거하는 방법과 같이 목표 변수를 사용한다.\n",
    "\n",
    "또 다른 방법으로는 <b>Wrapper</b>와 <b>Filter</b> 방법이 존재한다. 이 방법들은 변수 선택을 위해 사용되는 매커니즘 중 고려할 수 있는 Supervised Learning 방법이다.\n",
    "\n",
    "Wrapper 방법은 입력 변수들의 하위 집합이 서로 다른 많은 모델을 생성하고 성능 지표에 따라 최상의 성능을 보이는 변수를 선택한다. 이러한 방법은 계산상 비용이 많이 들 수 있지만 어떤 변수 형태에서도 사용이 가능하다. <b>RFE(Recursive Feautre Elimination)</b>는 Wrapper 방법 중 하나이다.\n",
    "\n",
    "<blockquote> Wrapper 방법은 입력 변수를 추가 및 제거하여 모델의 성능을 최대화하는 최적의 조합을 찾는 절차를 사용하고 여러 모델을 평가한다.</blockquote>\n",
    "— Page 490, Applied Predictive Modeling, 2013.\n",
    "\n",
    "Filter 방법은 통계적 기법을 사용하여 각 입력 변수와 목표 변수 사이의 관계를 평가하며, 평가한 점수는 모델에 사용될 입력 변수 필터링 기준으로 사용된다.\n",
    "\n",
    "<blockquote>Filter 방법은 예측 모델 외부의 입력 변수간 관련성을 평가하고 이후 일부 기준(평가 점수)를 통과한 입력 변수만 모델화 한다.</blockquote>\n",
    "— Page 490, Applied Predictive Modeling, 2013.\n",
    "\n",
    "마지막으로, 모델 학습의 일환으로 변수 선택을 자동으로 수행하는 머신러닝 알고리즘이 존재한다. 이러한 기법을 모델에 intrinsic(내장된) 변수 선택 방법이라고 할 수 있다.\n",
    "\n",
    "<blockquote> 일부 모델은 내장된 변수 선택을 포함하며, 이는 모델의 정확도를 극대화 하는 데 도움이 되는 입력 변수만 포함함을 의미한다. 이러한 경우 모델은 어떤 데이터가 가장 적합한지 선택할 수 있다.</blockquote>\n",
    "— Page 28, Applied Predictive Modeling, 2013.\n",
    "\n",
    "intrinsic 변수 선택 방법을 갖춘 알고리즘은 RandomForest와 같은 앙상블 및 의사결정나무 모델과 Lasso와 같은 규제화된 회귀 모델이 포함된다.\n",
    "\n",
    "<blockquote> 일부 모델은 예측에 유용하지 않은 정보를 가진 입력 변수들로부터 내성이 존재한다. 예를 들어, 트리 및 규칙 기반 모델, Lasso는 본질적으로 변수 선택을 수행한다.</blockquote>\n",
    "— Page 487, Applied Predictive Modeling, 2013.\n",
    "\n",
    "변수 선택은 예측에 불필요한 정보를 제거함으로써 차원 축소 기법과도 관련이 있다. 반면, 변수 선택은 데이터에서 유지하거나 제거할 변수를 선택하는 반면, 차원 축소는 데이터의 투영(Projection)을 생성하여 완전히 새로운 변수를 생성하는 것이다.\n",
    "\n",
    "구구절절 설명했지만, 변수 선택법은 다음과 같이 요약할 수 있다.\n",
    "\n",
    "- 변수 선택(Feature Selection): 주어진 데이터로부터 입력 변수의 하위 집합을 선택\n",
    "    - Unsupervised Learning: 목표 변수를 사용하지 않는다.(중복되는 변수를 제거한다.)\n",
    "        - Corrleation: 상관관계가 강한 입력 변수를 제거\n",
    "    - Supervised Learning: 목표 변수를 사용한다.(관련 없는 변수를 제거한다.)\n",
    "        - Wrapper: 최고의 성능을 내는 변수들에 대한 하위 집합을 선택\n",
    "            - RFE(Recursive Feature Elimination\n",
    "        - Filter: 목표 변수와 관계를 기준으로 변수의 하위 집합 선택\n",
    "            - 통계적 방법(Statistical Method)\n",
    "            - 변수 중요도(Feature Importance) 방법\n",
    "        - Intrinsic: 모델 훈련 과정에서 자동으로 변수 선택을 진행하는 알고리즘\n",
    "            - 의사결정나무\n",
    "            - Lasso 회귀\n",
    "            - Random Forest\n",
    "            \n",
    "- 차원 축소: 입력 데이터를 저차원 변수 공간에 투영\n",
    "\n",
    "<img src=\"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/11/Overview-of-Feature-Selection-Techniques3.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 통계학 기반 필터 Feature Selection 방법\n",
    "\n",
    "Filter기반 변수 선택법은 입력 변수와 출력 변수 사이의 상관관계를 통계적 측정하는 것이 일반적이다.\n",
    "\n",
    "통계적 측정을 이용한 변수 선택은 데이터의 유형에 크게 의존한다.\n",
    "\n",
    "공통 데이터 유형에는 수치형, 범주형이 포함되지만, 수치형 변수는 정수 및 실수, 범주형 변수는 부울, 순서형 또는 명목형 변수로 더욱 세분화될 수 있다.\n",
    "\n",
    "<img src=\"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2020/06/Overview-of-Data-Variable-Types2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "변수의 데이터 유형에 대해 더 많이 알수록, Filter 기반의 변수 선택 방법에 대한 적절한 통계적 측정을 선택하는 것이 더 쉽다.\n",
    "\n",
    "이 챕터에서는 입력 및 출력 두 가지 주요 변수 유형의 광범위한 범주를 고려한다.\n",
    "\n",
    "입력 변수는 모델에 대한 입력으로 제공되는 변수들이다. 변수 선택에서 크기를 줄이기 원하는 것은 입력 변수 그룹이다. 목표 변수는 종종 반응 변수라고 불리는 모델이 예측하려는 변수다.\n",
    "\n",
    "목표 변수의 유형은 일반적으로 수행중인 예측 모델링 문제의 유형을 나타낸다. 예를 들어, 수치형 목표 변수는 회귀 예측 모델링 문제를 나타내고, 범주형 목표 변수는 분류 예측 모델링 문제를 나타낸다.\n",
    "\n",
    "- <strong>수치형 출력</strong>: 회귀 예측 모델링 문제\n",
    "- <strong>범주형 출력</strong>: 분류 예측 모델링 문제\n",
    "\n",
    "Filter 기반 변수 선택에 사용되는 통계적 척도는 일반적으로 목표 변수를 사용하여 한 번에 하나의 입력 변수를 계산한다. 이 방법은 <strong>일변량 통계적 측정</strong>이라고 부른다. 이는 입력 변수 간의 상호작용이 Filtering 과정에서 고려되지 않음을 의미한다.\n",
    "\n",
    "<blockquote> 이러한 기법들은 대부분 일변량인데, 이는 각 입력 변수를 개별적으로 평가한다는 것을 의미한다. 이 경우 상관된 입력 변수가 존재하면 중요하지만 중복된 입력 변수를 선택하는 문제가 발생할 수 있다. 그 결과 너무 많은 입력 변수가 선택되어 다중공선성 문제가 발생할 수 있다.</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
